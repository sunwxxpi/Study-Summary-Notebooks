{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensor 객체의 Shape 관련 Function 및 Method 정리\n",
    "\n",
    "## 1. Shape 변경 (Memory 연속성 여부에 주의)\n",
    "\n",
    "### 1.1 `view`\n",
    "- **기능**: 텐서의 shape을 변경합니다.\n",
    "- **특징**: 데이터 순서는 그대로 유지하며, 연속적인 메모리 구조가 필요합니다. (차원이 재배열 된 경우에 view()를 사용하면 오류 발생)\n",
    "- **메모리**: 데이터 복사 없이 원본 데이터를 참조합니다.\n",
    "- **인자**: `*shape`\n",
    "  - 텐서의 새로운 shape을 정의하는 정수형 크기 값\n",
    "- **사용 예시**: 텐서를 `(4, 2)` shape으로 변경하기.\n",
    "  ```python\n",
    "  tensor = torch.arange(8)  # (8)\n",
    "  \"\"\"\n",
    "  tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "  \"\"\"\n",
    "  \n",
    "  reshaped_tensor = tensor.view(4, 2)  # (4, 2)\n",
    "  \"\"\"\n",
    "  tensor([[0, 1], \n",
    "          [2, 3], \n",
    "          [4, 5], \n",
    "          [6, 7]])\n",
    "  \"\"\"\n",
    "  ```\n",
    "\n",
    "### 1.2 `reshape`\n",
    "- **기능**: `view`와 비슷하게 텐서의 shape을 변경합니다.\n",
    "- **특징**: 메모리 구조에 덜 의존적이어서 더 유연하게 shape을 변경할 수 있으며, 필요시 데이터 복사가 일어날 수 있습니다.\n",
    "- **메모리**: 데이터가 연속적이라면 'view'를 반환하여 데이터 복사가 발생하지 않지만, 데이터가 불연속적이라면 데이터 복사 및 메모리 할당이 발생합니다.\n",
    "- **인자**: `*shape`\n",
    "  - 텐서의 새로운 shape을 정의하는 정수형 크기 값\n",
    "- **사용 예시**: 텐서를 `(2, 4)` shape으로 변경하기.\n",
    "  ```python\n",
    "  tensor = torch.arange(8)  # (8)\n",
    "  \"\"\"\n",
    "  tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "  \"\"\"\n",
    "\n",
    "  reshaped_tensor = tensor.reshape(2, 4)  # (2, 4)\n",
    "  \"\"\"\n",
    "  tensor([[0, 1, 2, 3], \n",
    "          [4, 5, 6, 7]])\n",
    "  \"\"\"\n",
    "  ```\n",
    "\n",
    "## 2. 기타\n",
    "\n",
    "### 2.1 `contiguous`\n",
    "- **기능**: 텐서가 연속된 메모리 위치에 저장되어 있는지 확인하고, 그렇지 않은 경우 새로운 연속적인 메모리를 할당하여 동일한 텐서를 반환합니다.\n",
    "- **특징**: 일부 텐서 연산 (예: `view`)은 텐서가 연속적 메모리 상에 있어야만 정상적으로 작동합니다. 만약 텐서가 메모리 상에 연속적이지 않다면, `contiguous()`를 사용하여 연속적인 메모리 형태로 변환할 수 있습니다.\n",
    "- **메모리**: 데이터 복사 및 메모리 할당이 발생합니다.\n",
    "- **사용 예시**:\n",
    "  ```python\n",
    "  tensor = torch.randn(2, 3, 4).permute(1, 0, 2)  # permute() 사용으로 인하여 메모리가 불연속적인 텐서\n",
    "  print(tensor.is_contiguous())  # False\n",
    "\n",
    "  contiguous_tensor = tensor.contiguous().view(4, 2, 3)  # 메모리가 연속적인 텐서로 변환 후 view() 사용 가능\n",
    "  print(contiguous_tensor.is_contiguous())  # True\n",
    "  ```\n",
    "\n",
    "### 2.2 `clone`\n",
    "\n",
    "- **기능**: 텐서의 데이터를 새로운 메모리 공간에 복사하여 원본과 동일한 속성을 갖지만 독립적인 텐서를 생성합니다.\n",
    "- **특징**: 원본 텐서가 변경되어도 클론된 텐서에는 영향을 주지 않습니다. `clone()`은 데이터만 복사하며, 연산 그래프와 연결 상태는 유지됩니다. 만약 연산 그래프와 분리하려면 `detach()`를 추가로 사용해야 합니다.\n",
    "- **메모리**: 데이터 복사 및 메모리 할당이 발생합니다.\n",
    "- **사용 예시**:\n",
    "  ```python\n",
    "  tensor = torch.arange(1, 7).view(2, 3)\n",
    "  \"\"\"\n",
    "  tensor([[ 1, 2, 3],\n",
    "          [ 4, 5, 6]])\n",
    "  \"\"\"\n",
    "\n",
    "  view_tensor = tensor.view(3, 2)  # 메모리 공유\n",
    "  cloned_tensor = tensor.clone()   # 메모리 독립적\n",
    "\n",
    "  tensor[0, 0] = 99\n",
    "\n",
    "  print(tensor)\n",
    "  \"\"\"\n",
    "  tensor([[99,  2,  3],\n",
    "          [ 4,  5,  6]])\n",
    "  \"\"\"\n",
    "  print(view_tensor)\n",
    "  \"\"\"\n",
    "  tensor([[99,  2],\n",
    "         [ 3,  4],\n",
    "         [ 5,  6]])\n",
    "  \"\"\"\n",
    "  print(cloned_tensor)\n",
    "  \"\"\"\n",
    "  tensor([[1, 2, 3],\n",
    "          [4, 5, 6]])\n",
    "  \"\"\"\n",
    "  ```\n",
    "\n",
    "## 3. 차원 재배열\n",
    "\n",
    "### 3.1 `permute`\n",
    "- **기능**: 텐서의 차원 순서를 변경합니다.\n",
    "- **특징**: 지정한 순서에 따라 차원을 재배열하며, 데이터는 그대로 유지됩니다.\n",
    "- **메모리**: 데이터 복사 없이 원본 데이터를 참조합니다.\n",
    "- **인자**: `*dims`\n",
    "  - 텐서의 차원을 재배열할 새로운 순서\n",
    "- **사용 예시**: `(2, 3, 4)` shape의 텐서를 `(2, 4, 3)` shape으로 차원 재배열하기.\n",
    "  ```python\n",
    "  tensor = torch.arange(24).view(2, 3, 4)  # (2, 3, 4)\n",
    "  \"\"\"\n",
    "  tensor([[[ 0,  1,  2,  3],\n",
    "           [ 4,  5,  6,  7],\n",
    "           [ 8,  9, 10, 11]],\n",
    "\n",
    "          [[12, 13, 14, 15],\n",
    "           [16, 17, 18, 19],\n",
    "           [20, 21, 22, 23]]])\n",
    "  \"\"\"\n",
    "\n",
    "  permuted_tensor = tensor.permute(0, 2, 1)  # (2, 4, 3)\n",
    "  \"\"\"\n",
    "  tensor([[[ 0,  4,  8],\n",
    "           [ 1,  5,  9],\n",
    "           [ 2,  6, 10],\n",
    "           [ 3,  7, 11]],\n",
    "\n",
    "          [[12, 16, 20],\n",
    "           [13, 17, 21],\n",
    "           [14, 18, 22],\n",
    "           [15, 19, 23]]])\n",
    "  \"\"\"\n",
    "  ```\n",
    "\n",
    "### 3.2 `transpose`\n",
    "- **기능**: 두 개의 차원을 교환하여 위치를 바꿉니다.\n",
    "- **특징**: `permute`와 유사하지만, 두 개의 차원만 바꿀 때 사용하기 간편합니다.\n",
    "- **메모리**: 데이터 복사 없이 원본 데이터를 참조합니다.\n",
    "- **인자**: `dim0`, `dim1`\n",
    "  - 교환하고자 하는 두 차원의 인덱스 값\n",
    "- **사용 예시**: `(2, 3, 4)` shape의 텐서를 `(2, 4, 3)` shape으로 차원 교환하기.\n",
    "  ```python\n",
    "  tensor = torch.arange(24).view(2, 3, 4)  # (2, 3, 4)\n",
    "  \"\"\"\n",
    "  tensor([[[ 0,  1,  2,  3],\n",
    "           [ 4,  5,  6,  7],\n",
    "           [ 8,  9, 10, 11]],\n",
    "\n",
    "          [[12, 13, 14, 15],\n",
    "           [16, 17, 18, 19],\n",
    "           [20, 21, 22, 23]]])\n",
    "  \"\"\"\n",
    "\n",
    "  transposed_tensor = tensor.transpose(1, 2)  # (2, 4, 3)\n",
    "  \"\"\"\n",
    "  tensor([[[ 0,  4,  8],\n",
    "           [ 1,  5,  9],\n",
    "           [ 2,  6, 10],\n",
    "           [ 3,  7, 11]],\n",
    "\n",
    "          [[12, 16, 20],\n",
    "           [13, 17, 21],\n",
    "           [14, 18, 22],\n",
    "           [15, 19, 23]]])\n",
    "  \"\"\"\n",
    "  ```\n",
    "\n",
    "## 4. 차원 추가 및 제거\n",
    "\n",
    "### 4.1 `squeeze`와 `unsqueeze`\n",
    "- **기능**:\n",
    "  - `squeeze`: 크기가 1인 차원을 제거합니다.\n",
    "  - `unsqueeze`: 지정한 위치에 크기가 1인 차원을 추가합니다.\n",
    "- **특징**: 데이터 구조는 그대로 유지하면서, 특정 차원의 크기를 1로 만들거나 제거할 수 있습니다.\n",
    "- **메모리**: 데이터 복사 없이 원본 데이터를 참조합니다.\n",
    "- **인자**: `dim`\n",
    "  - 제거하고자 하는 크기가 1인 차원의 인덱스, 지정하지 않으면 크기가 1인 모든 차원을 제거 (squeeze)\n",
    "  - 크기가 1인 차원을 추가하고자 하는 위치의 인덱스 (unsqueeze)\n",
    "- **사용 예시**:\n",
    "  ```python\n",
    "  tensor = torch.arange(1, 9).view(1, 2, 4)  # (1, 2, 4)\n",
    "  \"\"\"\n",
    "  tensor([[[1, 2, 3, 4],\n",
    "           [5, 6, 7, 8]]])\n",
    "  \"\"\"\n",
    "\n",
    "  squeezed_tensor = tensor.squeeze(0)  # (2, 4)\n",
    "  \"\"\"\n",
    "  tensor([[1, 2, 3, 4],\n",
    "          [5, 6, 7, 8]])\n",
    "  \"\"\"\n",
    "  unsqueezed_tensor = tensor.unsqueeze(2)  # (1, 2, 1, 4)\n",
    "  \"\"\"\n",
    "  tensor([[[[1, 2, 3, 4]],\n",
    "\n",
    "           [[5, 6, 7, 8]]]])\n",
    "  \"\"\"\n",
    "  ```\n",
    "\n",
    "## 5. Tensor 결합 및 확장\n",
    "\n",
    "### 5.1 `cat` (concatenate)\n",
    "- **기능**: 지정한 차원을 기준으로 여러 텐서를 연결합니다.\n",
    "- **특징**: 연결하려는 차원의 크기는 일치해야 합니다.\n",
    "- **메모리**: 데이터 복사 및 메모리 할당이 발생합니다.\n",
    "- **인자**:\n",
    "  - `tensors`: 연결하고자 하는 텐서들\n",
    "  - `dim`: 연결 기준이 되는 차원의 인덱스\n",
    "- **사용 예시**: `(2, 4)` shape의 텐서 두 개를 `(4, 4)`, `(2, 8)` shape으로 연결하기.\n",
    "  ```python\n",
    "  tensor1 = torch.arange(1, 9).view(2, 4)  # (2, 4)\n",
    "  \"\"\"\n",
    "  tensor([[1, 2, 3, 4],\n",
    "          [5, 6, 7, 8]])\n",
    "  \"\"\"\n",
    "  tensor2 = torch.arange(9, 17).view(2, 4)  # (2, 4)\n",
    "  \"\"\"\n",
    "  tensor([[ 9, 10, 11, 12],\n",
    "          [13, 14, 15, 16]])\n",
    "  \"\"\"\n",
    "\n",
    "  concatenated_tensor_0 = torch.cat((tensor1, tensor2), 0)  # (4, 4)\n",
    "  \"\"\"\n",
    "  tensor([[ 1,  2,  3,  4],\n",
    "          [ 5,  6,  7,  8],\n",
    "          [ 9, 10, 11, 12],\n",
    "          [13, 14, 15, 16]])\n",
    "  \"\"\"\n",
    "  concatenated_tensor_1 = torch.cat((tensor1, tensor2), 1)  # (2, 8)\n",
    "  \"\"\"\n",
    "  tensor([[ 1,  2,  3,  4,  8,  9, 10, 11],\n",
    "          [ 5,  6,  7,  8, 12, 13, 14, 15]])\n",
    "  \"\"\"\n",
    "  ```\n",
    "\n",
    "### 5.2 `stack`\n",
    "- **기능**: 여러 텐서를 새로운 차원에서 쌓아 결합합니다.\n",
    "- **특징**: 결합하려는 텐서들의 모든 차원이 동일해야 하며, 지정한 새로운 차원에서 스택을 형성합니다.\n",
    "- **메모리**: 데이터 복사 및 메모리 할당이 발생합니다.\n",
    "- **인자**:\n",
    "  - `tensors`: 결합하고자 하는 텐서들의 리스트\n",
    "  - `dim`: 새로 추가되는 차원의 인덱스\n",
    "- **사용 예시**: `(3, 4)` shape의 텐서 두 개를 `(2, 3, 4)` shape으로 결합하기.\n",
    "  ```python\n",
    "  tensor1 = torch.arange(1, 13).view(3, 4)  # (3, 4)\n",
    "  \"\"\"\n",
    "  tensor([[ 1,  2,  3,  4],\n",
    "          [ 5,  6,  7,  8],\n",
    "          [ 9, 10, 11, 12]])\n",
    "  \"\"\"\n",
    "  tensor2 = torch.arange(13, 25).view(3, 4)  # (3, 4)\n",
    "  \"\"\"\n",
    "  tensor([[13, 14, 15, 16],\n",
    "          [17, 18, 19, 20],\n",
    "          [21, 22, 23, 24]])\n",
    "  \"\"\"\n",
    "\n",
    "  stacked_tensor = torch.stack((tensor1, tensor2), 0)  # (2, 3, 4)\n",
    "  \"\"\"\n",
    "  tensor([[[ 1,  2,  3,  4],\n",
    "           [ 5,  6,  7,  8],\n",
    "           [ 9, 10, 11, 12]],\n",
    "\n",
    "          [[13, 14, 15, 16],\n",
    "           [17, 18, 19, 20],\n",
    "           [21, 22, 23, 24]]])\n",
    "  \"\"\"\n",
    "  ```\n",
    "\n",
    "### 5.3 `repeat`\n",
    "- **기능**: 텐서를 지정된 횟수만큼 반복하여 크기를 확장합니다.\n",
    "- **특징**: 데이터의 실제 복사가 일어납니다. 메모리를 추가적으로 사용하여 확장된 형태의 텐서를 생성합니다.\n",
    "- **메모리**: 데이터 복사가 발생하여 메모리 사용량이 증가합니다.\n",
    "- **인자**: `*repeats`\n",
    "  - 각 차원에 대해 반복할 횟수를 정의하는 정수형 크기 값\n",
    "- **사용 예시**: `(2, 3)` shape의 텐서를 `(6, 15)` shape으로 반복하기.\n",
    "  ```python\n",
    "  tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])  # (2, 3)\n",
    "  \"\"\"\n",
    "  tensor([[1, 2, 3],\n",
    "          [4, 5, 6]])\n",
    "  \"\"\"\n",
    "\n",
    "  repeated_tensor = tensor.repeat(3, 5)  # (6, 15)\n",
    "  \"\"\"\n",
    "  tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "          [4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6],\n",
    "          [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "          [4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6],\n",
    "          [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "          [4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6]])\n",
    "  \"\"\"\n",
    "  ```\n",
    "\n",
    "### 5.4 `expand`\n",
    "- **기능**: 텐서의 크기가 1인 차원을 확장합니다.\n",
    "- **특징**: 데이터 복사 없이 원래 텐서를 참조하는 형태로 확장됩니다. 크기가 1이 아닌 차원은 확장할 수 없습니다.\n",
    "- **메모리**: 데이터 복사 없이 브로드캐스팅된 방식으로 작동합니다.\n",
    "- **인자**: `*shape`\n",
    "  - 확장된 텐서의 최종 shape을 정의하는 정수형 크기 값\n",
    "- **사용 예시**: `(1, 3)` shape의 텐서를 `(4, 3)` shape으로 확장하기.\n",
    "  ```python\n",
    "  tensor = torch.tensor([[1, 2, 3]])  # (1, 3)\n",
    "  \"\"\"\n",
    "  tensor([[1, 2, 3]])\n",
    "  \"\"\"\n",
    "  expanded_tensor = tensor.expand(4, 3)  # (4, 3)\n",
    "  \"\"\"\n",
    "  tensor([[1, 2, 3],\n",
    "          [1, 2, 3],\n",
    "          [1, 2, 3],\n",
    "          [1, 2, 3]])\n",
    "  \"\"\"\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
